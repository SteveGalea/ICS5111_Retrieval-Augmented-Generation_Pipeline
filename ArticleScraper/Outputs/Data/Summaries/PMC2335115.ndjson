{"summary": "automatic thesaurus construction based on patterns was first suggested by Hearst [1] but it is still not clear how to automatically construct such patterns for different semantic relations and domains. the assumption of extant resources such as parsers is also a limiting factor for many languages, so it is desirable to find patterns that do not use syntactical analysis. auri, such as WordNet, give relatively poor coverage of specialised domains. thesauri and ontologies often do not give sufficient variants of terminology. the accuracy of such methods remains a major issue. parsers generated large number of patterns and classified them by a logistic regression-based method. they also used a number of hand-chosen synonymy patterns to detect potential synonyms. they also used a number of hand-chosen synonymy patterns to detect potential synonyms. we used the number of matches of each pattern for each term pair to create a feature vector. we then used probabilistic analysis to find the most likely set of synsets. bioCaster was developed for the search and analysis of internet news and research literature. the bioCaster database consisted of 450 terms, grouped into 244 synsets. thesauri can differ in their definitions of synonymy as investigated by Burgun & Bodenreider. many terms may turn out to be interchangeably as the difference in meaning is rarely important. 301 terms, 221 synsets and 101 synonym pairs shared with the BioCaster database. we extracted a large corpus from which we can build patterns and statistics. we chose the top 250 abstracts for each term ranked according to Entrez search's ranking. thesauri can differ in their definitions of synonymy as investigated by Burgun & Bodenreider. many terms may turn out to be interchangeably as the difference in meaning is rarely important. some terms may turn out to be used interchangeably as the difference in meaning is rarely important. terms and 16 synonym pairs with the BioCaster database. a second annotator group our 301 test terms into synonym groups showed = 68.6% Cohen's agreement with our list. we extracted a large corpus from which we can build patterns. #s, *s and (space)s can be expanded to give us a search. by considering each possible single expansion of a * or (space) our problem can be viewed as a tree search. the algorithm is as follows: Figure 1. Example Pattern Search. c e s m a t c h i n g p a t t e r n # s e n c e s i n c o r p u s. we also report a pseudo-F-Measure given as below. we judged a pattern match to be correct if it corresponded to a term in our training set. we used the top 6000 patterns from the pattern generation algorithm. this then becomes a standard statistical classification problem. this becomes a standard statistical classification problem. synset formation The results we gained from the statistical classification procedure gave only the probability of a particular term pair being synonymous. however we would expect every pair of terms in a synset to be synonymous. this is technically incorrect as some words may be polysemous. many of the output probabilities from our classifier were near 0 or 1. we also define the inter-node cost, cij as cij = log(Pij) + log(Pji) - log(1 - Pij) - log(1 - Pji) - log(1 - Pji) -iJ cik > maxV'ViV' cik Proof: Follows directly from the inequality c(K) c(Kk) + c(k) This is very useful as the set V' is simply the set for which cik is positive. it is possible to divide the problem by using the following lemma Lemma 2. for each connected component V in 1... N (a) Sort V by k = 1...Ncik (b) generate_matrix(J, V) function generate_matrix(J, V) i. for each connected component, C, with |C J| > 0 and |C V| > 0: generate_matrix(J, C V) ii. end Solving the synset problem. the branch and bound simply discards any partial solution when the most it costs is guaranteed to be less than the best solution found so far. more advanced algorithm use either linear relaxation (that is allowing x to take non-integer values and solving with the simplex algorithm) but we did not find this necessary for our data sets. pattern generation is as follows. we start with base patterns which consist of three basic operators *, #, and (space), where * represents a sequence of word characters. # represents any white-space/punctuation between words, and # represents a term. a set of patterns P sorted by a scoring metric 1. Add all the base patterns with a heuristic score to a heap H. 2. For a fixed number of iterations (a) Select a pattern, p, with maximum score in H. (b) Find all matches of this pattern using all term pairs in the corpus (c) e r n m a t c h e s # a l l p a t t e r n m a t c h e s r = # s y n o n y m p a i r s f o u n d # s y n o n y m p a we used the top 6000 patterns from the pattern generation algorithm. this then becomes a standard statistical classification problem. we would expect every pair of terms in a synset to be synonymous. this leads to the result that synsets are complete graphs. the graph above shows a graph representation of the output on top. it shows a graph representation of the output on top, where the nodes represents terms. the graph should give two synsets as shown in the bottom graph. we can greatly reduce this problem by finding a small set of potential synsets I such that I* I P(1..N) we find a condition when a set and all of its super-sets are not optimal by observing that J I* if c(J k) c(J) + c(k) for some k I. for each connected component V in 1... N (a) Sort V by k = 1...Ncik (b) generate_matrix(J, V) function generate_matrix(J, V) function generate_matrix(J, V) 1. For each connected component V in 1... N (a) Sort V by k = 1...Ncik (b) generate_matrix(J, V) function generate_matrix(J, V) function generate_matr solving the synset problem is the set covering problem, which is NP-complete, however we found exact methods to be suffcient for our problem. finding an exact cover is equivalent to finding the integer vector, x, which maximises cTx subject to Ax = 1. the branch and bound simply discards any partial solution when the most it costs is guaranteed to be less than the best solution found so far. the size of the search space is S i S B | S i. but by attacking each of the problems separately we get the size of the search space as S i S B | S i. the complexity of this problem is primarily dependent on the size of the largest connected component. meSH, UMLS and WordNet were queried using their built in tools. encyclopedia and thesauri made a number of mistakes. 78.3% of these non-occurring term pairs involved one or more term which matched less than 100 articles on PubMed. this suggests that this value may be very close to a limit of the recall of the method. we examined three more patterns from Yu et al which did not perform so well in our experiments. this was partly due to our syntax-free approach matching sub-terms. we then tried several statistical pattern recognition algorithms. each point corresponds to a pattern generated from the test set and its recall and precision when used as the only variable for a classifier. the pattern generation took 51 hours on an 1.66 GHz processor with 512 MB of memory. the difference is significant at a 99% level using the p-test as described in Yeh [26]. the data points are the synset solutions generated by the synset solver before finding the optimal solution and the optimal solution. erroneous links between terms are likely to be caused by artifacts in the data and method. erroneous links between terms are likely to be caused by artifacts in the data and method. we used encyclopedia and thesaurus to provide a comparison of our results. encyclopedia and thesaurus were not originally intended to be used as thesaurus. thesauri contains specialised thesaurus on the subject of human diseases. an \"Alternative Names\" section gave us synsets. this gave us a good set of medical terminology but it did not cover animals and animal diseases. the number of links correctly found are presented in Table 2. 78.3% of these non-occurring term pairs involved one or more term which matched less than 100 articles on PubMed. this suggests that this value may be very close to a limit of the recall of the method. # * (# * * 75.0 11.8% 13.4 4.0% 22.7 6.0% and hepatitis C virus infection (HCV) and *, * # * (# * * * 60.0 11.9% 13.4 4.0% 21.9 5.8% septicemia, anthrax, swine fever (hog cholera), and against # (* # 64.0 15.9% 7.9 3.1% 14.1 5.2% against enterotoxigenic Escherichia coli. pattern generation took 51 hours on an 1.66 GHz processor with 512 MB of memory. then a further 33 hours to generate feature vectors. the classification took 85 seconds and the synset formation 5 seconds. this was due to more positive links in this data set. 3.3 3.7% Nave Bayes 30.6 5.3% 37.5 7.1% 33.8 5.7% Logistic Regression 40.8 7.6% 29.7 6.6% 34.2 6.6% C4.5 74.1 11.7% 21.3 6.0% 33.1 7.8% SVM 82.2 10.5% 22.8 6.1% 35.7 7.9% Logistic Regression & Synset Formation 39.6 6.7% 35.6 6.9% 37.5 6.3% SVM & Synset Formation 73.2 9.8% 29.7% 42.3 7.7%. analysis of Errors Correct Modified Variant Binary 40.8% 1.4% 7.5% Synset 39.6% 0% 12.1% MoT/PoI Agent/Disease Hypernym Error 7.5% 12.9% 2.0% 29.3% 9.9% 17.6% 3.3% 17.6% Table 6 illustrate some sample output from the process. our pattern generation method developed a number of interesting patterns. it identified parentheses as the strongest indicator of synonymy. it also found several domain specific patterns suggesting the effectiveness of generating separate patterns for specific domains. we tested our method on only a limited domain but feel it would likely generalize well to other domains. our novel synset grouping method converted the result to something more applicable, but also improved on the results for both a strict definition of synonymy."}