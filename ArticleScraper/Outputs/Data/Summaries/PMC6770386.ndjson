{"summary": "clinical applications include outbreak investigation, viral typing and drug resistance analysis, and microbiome (amplicon-based) metagenomics. the shotgun metagenomics using NGS is a promising technique to analyze both the DNA and RNA microbial material from patient samples. WG suggested that SIB organizes a viral metagenomics ring trial. the viral metagenomics ring trial was designed to be a quality control test for pathogen identification from viral metagenomics data. the ring trial was designed to be a quality control test for pathogen identification from viral metagenomics data. participants received 13 FASTQ datasets for increment 2. output files are listed at the top after each workflow step. participants performed the bioinformatics analysis twice, once with a SIB-provided common database. participants returned: Filled questionnaire on the methodology that was used for each pipeline. participants returned the same output as for increment 1 except for the raw reads. the common database was provided in three versions. a viral multiplex control was prepared according to the protocol. the reagent contains a dilution of the following viruses in human plasma. a viral multiplex control was prepared according to the protocol. a NIBSC negative control [14] was prepared by pooling the content of two vials of 4 mL, mixed and 1 mL aliquots were stored at 20 \u00b0C until shipment on dry ice. the resulting five identically-looking samples were labeled with a number from one to five (Table 1) and shipped frozen to the contact person of each participating laboratory. we provided 13 FASTQ datasets to participants, five of which were obtained from real samples, and eight consisted of in-silico generated reads. all samples were relabled with a random number from one to 13 (Table 1, column \u201cLabel\u201d). pipeline H was the only one to sequence DNA and RNA separately. we devised two challenges, with four datasets each, representing in total eight datasets. participants were not informed that some datasets contained the same viruses. the first challenge, consisting of datasets \u201cII\u201d, contained a preset number of sequences from one strain of each of the following viruses. dilution datasets were re-generated from the test set FASTA sequences. each of these datasets was generated for several combinations of settings. the dataset generated for 1 150 bp 1 150 bp 1 250 bp 2 100 bp 2 150 bp 2 250 bp. bp contained only 67% of reads of viral origin as compared to the dataset generated for 1 100 bp (as 100/150 = 67%) fractions of contaminants of human and non-human origin were invariably fixed to 54% and 45% of the total number of reads. participants were asked to prefix all filenames using the convention (pipeline_letter) + (sample_number), e.g., A4.bam for the BAM file of sample 4 obtained with pipeline A. participants had to answer a short questionnaire on the methodology they used. the questionnaire was pipeline-specific and password-protected by a pipeline-specific token. questionnaire consisted of 24 questions covering: storage sample preparation (enrichment) DNA/RNA extraction, quantification, quality assessment Library preparation Sequencing Bioinformatics. typos in headers were manually corrected to match the expected headers. participants were not asked to mention in the virus metrics file whether they would report the virus or not. we used the NCBI accession number provided for each reported virus in the metrics file (i.e., \u201creport\u201d column set to \u201c1\u201d) to retrieve the virus full name using the NCBI E-utilities API. the virus name was then mapped against the regular expressions representing the expected positives (Table S1) the ring trial was designed to disentangle the variability arising from the wet lab compared to the bioinformatics parts of the pipeline. participants performed the bioinformatics analysis twice, once with a SIB-provided common database, and once using their own in-house database. five samples consisting of the human blood plasma spiked with known viruses (cf. Section 2.3). SIB common database (cf. Section 2.2). participants returned: Filled questionnaire on the methodology that was used for each pipeline (done only once when first joining the RT, either at increment 1 or 2). raw reads from the sequencer (FASTQ), before further pre-processing. participants were already asked to compute summary statistics including read counts in the virus metrics file. clades containing fewer than two entries were dropped. no clade was allowed to exceed 20 entries (20 entries were drawn at random from clades with 21 or more; excess entries were discarded) only the training set was provided to participants. the common database was provided in three versions. total nucleic acid was extracted with the NucliSENS eMAG (bioM\u00e9rieux, Marcy l\u2019Etoile, France) followed by qPCR in triplicates with specific primers. a viral multiplex control [13] from the national institute for Biological Standards and Control (NIBSC) was prepared according to the protocol. in increment 2, participants received the same set of sequencing reads in FASTQ format, which they subjected to the bioinformatics part of their pipeline. all samples were relabled with a random number from one to 13. four datasets representing in total eight datasets. participants were not informed that some datasets contained the same viruses. the first challenge contained a preset number of sequences from one strain of each of the following viruses. we then performed a 1:40 fold serial dilution, a 1:40 fold serial dilution increasing the mutation rate in art_illumina, and a 1:400 fold serial dilution to obtain four datasets. the number (nb) of reads per virus was set here for the reads of length 150 bp. for the other read lengths, please refer to Table S2. each of these eight datasets was generated for several combinations of settings (single (one) compared to paired-end (two) read length) participants could choose the settings corresponding to their actual workflow. however, in order to ensure fairness among participants, we decided to normalize the number of reads per virus to the read length. pipeline H was the only one to sequence DNA and RNA separately. we selected the five datasets among the data provided by the remaining three sequencing centers (i.e., pipelines A, E, I), to make sure every participating laboratory could use their existing workflow without having to integrate results from two separate analyses. human rubulavirus 4 (HPIV-4) Human orthopneumovirus (Human respiratory syncytial virus) We then performed a 1:40 fold serial dilution, a 1:40 fold serial dilution increasing the mutation rate in art_illumina, and a 1:100 fold serial dilution to obtain four datasets. each of these eight datasets was generated for several combinations of settings. participants had three months to perform increment 1 (February to May 2018) and 2.5 months to perform increment 2 (july to September 2018). participants were asked to use identical settings for a given pipeline. four laboratories participated in increment 1, for a total of seven pipelines. five laboratories participated in increment 2, for a total of seven pipelines. participants were asked to prefix all filenames using the convention (pipeline_letter) + (sample_number), e.g., A4.bam for the BAM file of sample 4 obtained with pipeline A. participants had to answer a short questionnaire on the methodology that they used. participants were not asked to mention in the virus metrics file whether they would report the virus or not. if the NCBI accession number was mentioned in the report, we simply set the corresponding virus in the metrics file as \u201c1\u201d in the \u201creport\u201d column. reports mentioning \u201cUninterpretable\u201d or \u201cCannot be excluded\u201d were considered as not reported. performance was measured as the F1-score, which is the harmonic mean of sensitivity and precision. sensitivity is measured as TP/(TP + FP), where TP are the true positives and FN are the false negatives. sensitivity reflects the fraction of true positives among all the predicted positives. if the NCBI accession number was mentioned in the report, we simply set the corresponding virus in the metrics file as \u201c1\u201d in the \u201creport\u201d column. reports mentioning \u201cUninterpretable\u201d or \u201cCannot be excluded\u201d were considered as not reported. however, some reports did not mention at all the NCBI accession numbers but only a virus name. performance was measured as the F1-score, which is the harmonic mean of sensitivity and precision. sensitivity, also known as recall or true positive rate, is measured as TP/(TP + FN), where TP are the true positives and FN are the false negatives. sensitivity reflects the fraction of true positives among all the predicted positives. pipelines based on mapping achieved a better performance than k-mers. performance on the NIBSC multiplex viral control was in general poorer. but virus concentrations were in similar range than for 1:100 dilution. we plot the F1-score obtained by each pipeline on the four spiked biological samples provided in increment 1. we also plot the F1-score on the same samples but starting this time from one common FASTQ dataset. pipeline A did not report any results for two of these samples in increment 2. pipeline I had identified viruses in various settings (1100 bp, 1 150 bp, 1 250 bp), which correspond to the sequencing settings they used in increment 1. all the identified viruses in samples of increments 1 and 2 are available as Figures S3\u2013S19. results show participants consistently performed much worse with sample NIBSC_multiplex_b than with sample NIBSC_multiplex_a. Interestingly, the average performance of all pipelines on NIBSC_multiplex_a was similar to that of pipeline I at increment 1. the average performance of all pipelines on NIBSC_multiplex_b was similar to that of pipeline E at increment 1. we still observe great variability in the F1 performance at increment 2 across all the pipelines, which is driven by a high variability in precision. this suggests that the bioinformatics methodology also has a strong impact on overall performance, in particular the capability of avoiding calling false positives. this suggests that sequencing should be deep enough to achieve a 10 coverage of viruses. all the pipelines as present in the negative sample were reported by only some of the pipelines. pipeline H only reported the two families of viruses reported by everyone and no other false positive. this suggests that these may arise from the bioinformatics workflow per se. given pipeline using the SIB compared to the in-house database (Mann-Whitney U test), suggesting that in our RT, it is the mapping/classification algorithm that is driving the overall performance. most of the F1-score signal was driven by a differential performance in precision across the pipelines, rather than sensitivity. pipelines H and I had a sensitivity and precision of 100% on all three dilutions of the spiked samples, resulting in an F-score equal to one. pipelines based on mapping achieved a better performance than those based on k-mers. pipeline I achieved different performances with the same FASTQ dataset at increment 1 compared to 2. we plot the F1-score obtained by each pipeline on the four spiked biological samples provided in increment 1. we report the F1-score obtained by each pipeline on artificial datasets. participants sometimes reported no viruses at all in some of the artificial samples. we could therefore not calculate an F1-score for these samples. results show participants consistently performed much worse with sample NIBSC_multiplex_b than with sample NIBSC_multiplex_a. the average performance of all pipelines on NIBSC_multiplex_a was similar to that of pipeline I at increment 1. the average performance of all pipelines on NIBSC_multiplex_b was similar to that of pipeline E at increment 1. the increment 2 Spiked_1-10 dataset was provided by laboratory E. the increment 2 Spiked_1-10 dataset was provided by laboratory I. the increment 2 Spiked_1-100 dataset was provided by laboratory I. this is driven by a high variability in precision. this suggests that the bioinformatics methodology also has a strong impact on overall performance. pipeline H only reported the two families of false positive viruses reported by everyone and no other false positive. pipeline C is absent as it only returned results using its own in-house database, and not with the SIB common database. Figure 6 shows the F1-score obtained by the five pipelines that participated in increment 1. these results show no significant difference in performance for any given pipeline using the SIB compared to the in-house database. this suggests that in our RT, it is the mapping/classification algorithm that is driving the overall performance rather than the reference sequences database that is used along the algorithm. our results have highlighted various aspects that can impact the overall performance at the experimental, databases and bioinformatics levels. our results have highlighted various aspects that can impact the overall performance at the experimental, databases and bioinformatics levels. in order for others to benefit from this study and test their own workflows, we have published with this manuscript the database and all the datasets that were generated. the reported accession number will be useful to retrieve the virus full name. it is also essential that the list of allowed names for each of the expected viruses at the chosen resolution level is made available after such proficiency tests for participants review. participants generated their results using a common, SIB-provided database. one of the aims was to assess the impact of the database compared to the algorithms on performance. participating laboratories have hired bioinformaticians and dedicate resources in (semi-automatically/automatically) refining their internal databases. the NIBSC viral multiplex control clearly did not come close to a real clinical scenario. this may explain the observed much lower performance on these samples due to the high number of viruses in the sample. this may explain the observed much lower performance on these samples due to the high number of viruses in the sample. the impact of human review and the observation that one pipeline did not reproduce its findings starting from an identical FASTQ dataset also calls for individual laboratories more clearly defining internal standard interpretation criteria. participants of the ring trial did not have the possibility to use negative controls that were representative of the samples being tested, since they lacked this information. ring trials are pilot studies paving the way to standardizing quality controls in clinical viral metagenomics. these initiatives are pilot studies paving the way to standardizing quality controls in clinical viral metagenomics. we want to emphasize the need to use NCBI accession numbers together with regular expressions. in clinical practice, laboratories use different names for the same virus. in clinical practice, laboratories use different names for the same virus. this is of particular importance since viral taxonomy is not consistently defined. this is of particular importance since asking for virus identification may mean different taxonomic names and levels for different laboratories depending on the virus. participants generated their results with a common, SIB-provided database. the database enabled participants to get as good results as they would get using their own in-house database. it is unclear how generalizable this finding may be, given the limited number of tested viruses in our samples. the ring trial was designed in such a way that the ground truth was always known in both increments. this came with the caveat that samples did not necessarily reproduce clinical conditions. the NIBSC viral multiplex control clearly did not come close to a real clinical scenario. a depth of coverage of 10 and beyond was associated with top performance. this finding may question the utility of performing ring trials assessing only one part of the workflow, by e.g., starting from common FASTQ datasets. this also highlights that participants should be cautious when analyzing data that do not originate from their own laboratory. laboratory contaminants and type-I errors may affect the participants\u2019 false positive rate. the increasing number of laboratories implementing NGS for viral metagenomics calls for quality management and the implementation of ring trials to ensure comparable results across sites in terms of performance, within clinically-realistic costs and turnaround times. it would be important to consider implementing these ring trials in an ISO-accredited framework. it would be important to join forces with the ISO17043 certified organizations currently running EQAs in clinical microbiology."}