{"summary": "phylogeography is growing increasingly popular among public health researchers. it allows researchers to estimate the origin and drivers of infectious diseases. it has recently been applied to study outbreaks of Ebola [2], avian influenza [3], and Zika virus [4], among others. resolution is usually performed in two steps. the first step detects and extracts all toponyms mentioned in the article. the second step disambiguates them with their unique coordinates. our work will improve phylogeography models for tracking evolutionary changes in viral genomes. the addition of more precise geographic metadata in building such models could enable health agencies to better target areas that represent the greatest public health risk. the gold standard corpus contains 60 articles from PubMed Central [13] all articles in our corpus are indexed in PubMed Central with links to the Influenza A virus entry in the NCBI\u2019s taxonomy database. two graduate students manually annotated the corpus. a Conditional Random Fields Classifier was used to improve toponym detection. we used the python-based CRFSuite-sklearn library [27] for training and feature analysis. toponym mentions can consist of single tokens (e.g., Phoenix) or multiple tokens. Lexical features encapsulate various lexical properties of the token to be classified. these include, the token itself and n surrounding tokens. knowledge-based features are generated via the use of several sources of knowledge. the semantic types of the concepts are added as features to the CRF. the lexical representations of the concepts and the semantic types are added as features to the CRF. techniques have been proposed to learn vector-based representations of texts. our method relies on the metadata associated with GenBank records to find occurrences of toponyms in sentences of PubMed articles linked to the records. if a researcher is uploading a new genetic sequence to the database, they are asked to provide additional metadata about the sequence. this includes the research article where the sequence is described and, when available, the infected host (human or animal) negative examples of toponyms appear in the sentences where positive examples of toponyms were found. for each word in this set we ran a statistical t-test to determine if the word was correlated with the toponyms in our corpus. lexical features perform significantly better than others by obtaining significantly high F-scores. overlapping evaluation considers partial overlap to be correct. strict evaluation evaluates each token separately. the figure shows that precision, recall, and F-score values are not significantly affected by the size of the training data. this is because once the supervised learning algorithm is exposed to examples of locations, it is able to correctly identify them on most occasions. the trend line indicates gradual improvements even at 150% of the data. the precision of our method on this sample was 0.84 with a total number of 502 occurrences and only 78 False Positives (FP), i.e. occurrences which were not considered toponyms by the annotator. the majority of the errors were made on one article in which the same virus name is repeated 34 times. a closer inspection of the first 50 sentences extracted as negative examples shows that the sentences tend to be short, ungrammatical and repeated. despite these limitations, the number and the quality of the examples extracted are high enough to train a classifier for NE detection. a NBC is fast to train on a large set of training examples and can perform well under certain conditions. a NBC is fast to train on a large set of training examples and can perform well under certain conditions [35]. data and annotations We used our gold standard corpus of 60 articles from PubMed Central [13] all articles in our corpus are indexed in PubMed Central with links to the influenza A virus entry in the NCBI\u2019s taxonomy database. we identified china as the most frequent toponym with 209 occurrences in our corpus. we modeled the problem as a sequence classification problem. we used the python-based CRFSuite-sklearn library. the features we used can be broadly grouped into three categories: lexical, knowledge-based and semantic. Lexical features encapsulate various lexical properties of the token to be classified. these include, the token itself and n surrounding tokens. generalized concept types (UMLS CUIs) and semantic types are added as features to the CRF. lexical representations of the concepts and the semantic types are added as features. our method relies on the metadata associated with GenBank records to find occurrences of toponyms in sentences of PubMed articles linked to the records. in the article, we considered all occurrences of the place of the infected host as positive examples of a toponym. we extracted 89,541 occurrences which we assume to be positive examples of toponyms. negative examples are phrases which are names of places found in GeoNames. but they are clearly not toponyms in the sentences where the phrases appear. for each word in this set we ran a statistical t-test. we evaluated performance via strict and overlapping matching [33]. for overlapping matching, each token is treated separately and the classifier is deemed to be correct when the actual and predicted tag for a token match exactly. we assess the performance of the classifier for the positive classes only (e.g., B and I) RF-Knowledge 0.78 0.35 0.49 0.76 0.34 0.47 CRF-Semantic 0.86 0.19 0.31 0.84 0.18 0.30 CRF-All 0.86 0.77 0.81 0.85 0.76 0.80 CRF-All (Macro) 0.85 0.75 0.80 0.84 0.74 0.79 Nave Bayes Classifier 0.52 0.89 0.66 0.51 0.86 0.64 Analysis To assess how performance is dependent on training set size, we performed ablation experiments using fractions 78 errors can be divided in 4 categories. first category is the use of a location name as adjective in a virus name. second category, with 22 cases, is a location used as a part of an institute name. our method exploits only lexical and syntactical clues. the training examples extracted with distant supervision exhibit a good quality. a closer inspection reveals the limitations of our method. a NBC is fast to train on a large set of training examples. the classifier categorizes all phrases that are not found in GeoNames as \u201cnot toponyms\u201d. a NBC is fast to train on a large set of examples. phylogeographers need precise locations of hosts infected by a virus of interest to build models of its origin and spread. toponym detector based on Conditional Random Fields exploits various categories of features to label jointly all toponyms occurring in a sentence. in this study, we used the country metadata field in GenBank to find toponyms in articles. other fields such as host, strain, species or authors can be used to extract other types of NEs."}