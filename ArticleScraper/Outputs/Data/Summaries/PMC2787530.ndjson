{"summary": "healthMap is part of a new generation of online systems designed to monitor and visualize disease outbreak alerts as reported by online news media and public health sources. a typical approach to geo-parsing would require an expensive training corpus of alerts manually tagged by a human. healthMap provides a starting point for real-time intelligence on emerging infectious diseases. the system uses a rule-based approach relying on a specially crafted gazetteer. the system uses a specially crafted gazetteer to add relevant geographic phrases. a look-up tree algorithm tries to find a match between the sequences of words in the alert and the sequences of words in the entries of the gazetteer. it also implements a set of rules which use the position of the phrase in the alert to decide whether or not the phrase is related to the reported disease. the current limited gazetteer has proven useful for a high level view of ongoing threats. our approach simulates this situation with a learning algorithm in the guise of an artificial \"reader\" some of the location words in the training texts are purposely hidden from the reader's vocabulary in order to divert the attention of the learning algorithm to the context on which these words appear. large numbers of unlabeled texts are easily available through, for example, the internet. a number of approaches, sometimes referred to as Automatic Knowledge Acquisition, have been developed. the aim of all these approaches is to exploit the redundancy of language. healthMap surveillance so far covers alerts in 4 more languages. most of these approaches, [15,16,19,20], use elaborate linguistic knowledge either to represent the words or to target groups of words to tag. our approach relies on low level syntactic features. our dataset (or corpus) is composed of P examples (x1, y1),..., (xP, yP) each example (x, y) is composed of an alert x = [x1, y1) with a certain number L of words xi. their corresponding \"location\" labels y = [y1,..., yL], yi loc, none. how we choose which words to hide is critical to the generalization power of our learning algorithm. naively deciding to hide the words tagged as location by the healthMap gazetteer would be equivalent to giving away the label information to the learning algorithm. s roughly 10% of the words in the corpus, but close to 25% of the location word occurrences. the list of words present in the training set represents the largest vocabulary the algorithm has access to. this would seem a good idea to diversify and augment this vocabulary. the percentage of location words is computed with respect to the locations found by the commercial geo-parser. a threshold on NN(i, x) allows us to decide if the input is a location or not. we used a comprehensive (subscription only) commercial geo-parser to tag 500 alerts with what we would consider \"true\" location references. both geo-parsers agree on 38.7% of the tags and the remaining 8.9% come from the HealthMap gazetteer. 201,643 words to tag among which 5,030 are considered locations by the healthMap gazetteer approach. 7,385 are considered locations by the commercial geo-tagger, of which 3,315 by it alone. 960 words are considered locations only by the healthMap gazetteer approach. the overall performance of the system decreases (solid red line) however, the increase in out-of-dictionary examples greatly improves the ability of the system to correctly identify locations that are out of the algorithm's dictionary (dash-dotted blue line) the idea behind this approach is to consider those purposely \"hidden\" location words as surrogates of the location words unknown to the gazetteer, those we want to be able to discover. performances of models trained on T0 (1,000 alerts), T1 (2,500 alerts), T2 (5,000 alerts) and T1 with location and disease targets. the core idea behind our approach is to have a dataset of alerts tagged with the gazetteer-based algorithm and then to use this dataset with tags partially hidden. the healthMap rule-based approach to tag the words in the alerts is a part-of-speech tagger. each word in the dictionary, its part-of-speech in the text and its capitalization status are associated with 3 indexes in the representation. if a word is out of the dictionary, it will only be represented by its part-of-speech and capitalization status. this is implemented by the choice of the dictionary D mentioned in the previous paragraph. naively deciding to hide the words tagged as location would be equivalent to giving away label information to the learning algorithm. the percentage of hidden words out of all words in the corpus is lower than the percentage of hidden location words out of all location words (second bar) the graph shows that cutting words that appear with a frequency lower than 2.8 1e-5 hides roughly 10% of the words in the corpus, but close to 25% of the location word occurrences. a new tab shows the number of out-of-vocabulary words among the words of the corpus. the second bar shows the number of location words outside the vocabulary. the dictionary size corresponding to is reported. we used a comprehensive (subscription only) commercial geo-parser to tag 500 alerts with what we would consider \"true\" location references. some of the tags are due to minor uninteresting variations in the annotation schema. some of the tags are due to minor uninteresting variations in the annotation schema. the evaluation corpus contains 500 disease outbreak alerts. this represents 201,643 words to tag among which 5,030 are words. 7,385 are considered locations by the commercial geo-tagger, of which 3,315 by it alone. the value of increases, the size of the dictionary we allow the NN geo-parser to see decreases. the increase in out-of-dictionary examples greatly improves the ability of the system to correctly identify locations that are out of the algorithm's dictionary. however, the increase in out-of-dictionary examples greatly improves the ability of the system to correctly identify locations that are out of the algorithm's dictionary. performance of models trained on T0 (1,000 alerts), T1 (2,500 alerts), T2 (5,000 alerts) and T1 with location and disease targets. Table 1 shows best performances with respect to MetaCarta labels. we have demonstrated that the described model has the ability to discover a substantial number of geographic references that are not present in the gazetteer. however, there is still a portion of those geographic references that remains a challenge to retrieve. a more sophisticated word-withholding strategy could be implemented. using alerts retrieved by HealthMap we generated a dataset specially tailored to train a geo-parsing algorithm. this algorithm was first applied to this set of alerts in order to extract the words in the text referring to geographic locations. the same alerts were then run through the part-of-speech tagger algorithm provided by NEC's project SENNA [22], making the syntax of the text explicit. the neural network was trained by negative log-likelihood minimization using stochastic gradient descent. the neural network with an example of input (with hw = 2) is illustrated in Figure 7. each word in the window sequence is given as input to a unique multi-layer perceptron. this threshold and the hyper-parameters of the neural network are tuned on a separate validation set. tuning this threshold away from 0.5 compensates for the fact that some none labels are in fact locations unknown to the HealthMap gazetteer."}