{"summary": "the higher the suspicion, the more likely it is that the result will confirm the diagnosis. this relationship can be used to improve laboratory testing by making it possible to estimate the pre-test probability of getting a given test result before ordering the test. the relationship between clinical suspicion and pre-test probability is used routinely to help set guidelines regarding when and when to order a given test. we used four types of input\u2014age, gender, diagnoses, and results of laboratory tests on blood samples added to the record in the seven days before a given test was ordered. each blood test (the test of interest), over 69.4 million in all, was marked as an in-house test (performed at the hospital) or a sendout (performed off-site) for tests with three values, we performed two separate rule searches. one for high vs. not high\u2014i.e., grouping normal and low\u2014and one for low vs. not low. we used GLM twice to find rules based on just those items that were common to rules found from a number of different training sets. the significance threshold was corrected for multiple comparisons by dividing the traditional threshold of p = 0.05 by the product of the total number of tests considered. the combined total number of features (in-house tests plus sendout tests plus diagnoses) was 170+81+434 = 685. the average number of rules after application of GLM for the first time for each test is 6. a 60-40 split generated a total number of rules comparable to 70-30 and 80-20 splits but with less training data. for each test of interest, we selected features that appeared in a strict majority of rules for that test and reran glm using only those features. the final models were tested on the test data and performance statistics are found. data-processing was performed in Python (Enthought Canopy Python version 2.7.3) R (version 2.15.3) was used for statistical analysis and reports generation. we used GLM twice: first to find rules based on a particular training set. second time to find rules based on just those items that were common to rules found from a number of different training sets. the significance threshold was corrected by dividing the traditional threshold of p = 0.05 by the product of the total number of tests considered. the combined total number of features (in-house tests plus sendout tests plus diagnoses) was 170+81+434 = 685. the average number of rules after application of GLM for the first time for each test is 6. a 60-40 split generated a total number of rules comparable to 70-30 and 80-20 splits but with less training data. for each test of interest, we selected features that appeared in a strict majority of rules for that test and reran glm using only those features. the final models were tested on the test data and performance statistics are found. data-processing was performed in Python (Enthought Canopy Python version 2.7.3). R (version 2.15.3) was used for statistical analysis and reports generation. results to determine how well sendout and in-house test results can be anticipated based on basic information available in the medical record. UC for most of these rules was low, with only five tests having AUC0.75: free T3, alpha-macroglobulin, CA27-29, hyaluronic acid, and alpha fetoprotein (AUC 0.75\u20130.79). a total of 170 in-house tests were analyzed. the best predictor of a result being normal or abnormal was whether it had been normal or abnormal within the previous seven days. PPV, NPV, and key predictors for selected tests were shown. PPV and NPV were better at finding rules with high positive predictive value (PPV; panels a and b), with good agreement between the methods. logistic regression [16]\u2013[18], a special case of generalized linear modeling (GLM) researchers have applied these approaches for diverse health-related purposes including prediction of cardiovascular risk [19], mortality in head trauma [18], texture analysis of magnetic resonance images [17], [20], [21], and many other applications [17], [20], [21]. however, GLM does not easily incorporate missing values, as it removes records with missing features. data-mining can sometimes find spurious correlations, artifacts of the particular partitioning of the data into training and test set. we repeated our regression on multiple independent partitions of the data and kept only items that appeared in a majority of the resulting rules. the effect on performance was negligible and dependence on the resulting items was more often clinically and pathophysiologically plausible than rules derived from each run. a NPV of 0.95 means that when a rule suggests that the test result will be normal, the result actually will be normal 95 percent of the time. the main determinant for rules for in-house tests was a normal or abnormal result for the same test within the previous seven days. the main determinant for rules for in-house tests was a normal or abnormal result for the same test within the previous seven days. rules will correctly predict an abnormal laboratory result 5 times out of 6 (5/60.84) and a normal result 3 times out of 4. these observations raise the question of how much better prediction can get."}