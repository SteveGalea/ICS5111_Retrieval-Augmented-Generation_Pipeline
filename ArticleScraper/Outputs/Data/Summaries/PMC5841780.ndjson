{"summary": "convolutional neural network applied to dermoscopy images of acral melanoma and benign nevi on the hands and feet. melanoma is the most common type of melanoma in Asians. it usually results in a poor prognosis due to late diagnosis. artificial intelligence and deep-learning models have been applied to help physicians who are untrained to handle a digital dermoscope. it has already shown potential for general and highly variable tasks across many fine-grained object categories[8\u201312] and has been shown to exceed human performance in object recognition[9]. a total of 724 dermoscopy images were collected from January 2013 to March 2014 at the Severance Hospital in the Yonsei University Health System, Seoul, Korea. 350 dermoscopy images were from 81 patients with AM and 374 images were from 194 patients with BN of the acral area. we fine-tuned a modified VGG model with 16 layers (13 convolutional and three fully connected layers) each layer and feature map in the CNN is represented by a three-dimensional array of size h w d >, where h and w are spatial dimensions, and d is the number of channels or feature dimensions. 512 1414512 conv 33512 1414512 max-pooling FC6 fully-connected 114096 4096 FC7 fully-connected 112 2 soft-max activation function. output classes were set to 2 (melanoma and non-melanoma classes) for the dermoscopic images. we froze the weights of conv1, conv2, conv3, conv4, and the first layer of conv5. after several training epochs, we trained the all weights of our network without freezing any layer. 2.2 Training and inference Our dataset consisted of 724 images and associated labels, which were split into two mutually exclusive subsets. melanoma classification shows training (upper) and inference (lower) stages. we randomly selected 30% of the training dataset as a validation set. the rest as a training set at the onset of training. validation data were used to prevent the overfitting of the training data. model with 16 layers downloaded from http://www.vlfeat.org/matconvnet/pretrained. a) 64 learned filters at the first layer, (b-m) 100 filters among the learned filters from the 2nd to 13th layers respectively. v e + t r u e n e g a t i v e + t r u e n e g a t i v e + f a l s e p o s i t i v e + f a l s e p o s i t i v e + f a images of BN were divided into nine types, and AM images into three types according to the reference[15], by two dermatologists. the protocol was approved by the Institutional Review Board of Yonsei University, Severance Hospital and Keimyung University, Dongsan Hospital. the number of output classes was set to 2 (melanoma and non-melanoma classes) for the dermoscopic images. the input with a fixed-size, 224 224, was passed through a stack of convolutional layers. each followed a rectified linear unit activation function. the first 2 fully connected layers (FC6 and FC7) had 4,096 channels each. the last layer had the soft-max activation function and predicted whether the input patch was a melanoma or non-melanoma lesion. after several training epochs, we trained the all weights of our network without freezing any layer. fig 3 shows 64 learned filters at the 1st convolutional layer. each represents a learned filter with a 3 3 kernel size. the input of the first layer is an RGB image with 224x224. 64 learned filters at the first layer, (b-m) 100 filters among the learned filters from the 2nd to the 13th convolution layer, respectively. the output feature maps are used as the input of the next layer. our network configuration is depicted in Fig 2 and Table 1. each layer and feature map in the CNN is represented by a three-dimensional array of size h w d >. h and w are spatial dimensions, and d is the number of channels or feature dimensions. the number of output classes was set to 2 (melanoma and non-melanoma classes) for the dermoscopic images. the input with a fixed-size, 224 224, was passed through a stack of convolutional layers, where each followed a rectified linear unit activation function. max-pooling was performed over a 2 2 pixel window with a stride of 2. training and inference Our dataset consisted of 724 images and associated labels. half of the total image dataset was selected for training and the rest for testing. the scale and location of a skin lesion in a captured image were changed. the validation error on the validated dataset stopped decreasing. we trained the network using an adaptive stochastic sub-gradient method. the batch size is set to 50, and the momentum parameter, learning rate, and weight decay are set to 0.9, 0.0001, and 0.0005, respectively. ness of the CNN compared its diagnostic rate with those of two dermatologists who had five or more years of clinical experience in dermoscopy (expert group) and two non-trained general physicians (non-expert group) all images on the computer screen were evaluated simultaneously. if there was a dissensus between two physicians, they reached a conclusion. Cohen \u2019 s Kappa = P o P e 1 P e (3) (Po = Accuracy, Pe = hypothetical probability of a chance agreement) results Among 724 dermoscopy images, 71 images were from the hands and fingers. 374 BN images included parallel furrow, fibrillar, lattice-like, reticular, globular, and homogenous patterns. youden's index showed 0.6795 in group A and 0.6073 in group B. the expert was higher than non-expert (group A: 0.6358, group B: 0.2509) and higher than non-expert. expert 77.10 70.24\u201383.03 PPV (%) CNN 73.97 69.55\u201377.95 Expert 73.50 69.39\u201377.25 Non-expert 67.20 60.05\u201373.64 NPV (%) CNN 90.37 84.64\u201394.12 Expert 97.50 92.67\u201399.18 Non-expert 60.26 56.30\u201364.10 Accuracy (%) CNN 80.23 75.77\u201384 CNN and Expert (95% confidential interval) CNN and Non-expert (95% confidential interval) Group A 0.5929 (0.5099\u20130.6760) 0.2620 (0.1868\u20130.3373) 0.2496 (0.1808\u20130.3185) Group B 0.6513 (0.5692\u20130.7335) 0.1972 (0.1109\u20130.2836) 0.1999 (0.1189\u20130.2811) The results are shown in Table 4. Table 4. Comparison metrics among CNN, Inception-V3 with a single image and Inception 69.12\u201381.62 PPV CNN 77.14 72.46\u201381.24 Inception-V3 (s) 76.63 71.38\u201381.17 Inception-V3 (m) 76.26 71.33\u201380.58 NPV CNN 91.88 86.95\u201395.05 Inception-V3 (s) 81.62 76.49\u201385.84 Inception-V3 (m) 85.96 80.72 NPV 83.37 84.64\u201394.12 Inception-V3 (m) 89.50 84.36\u201393.09 Accuracy CNN 80.23 75.77\u201384.04 Inception-V3 (m) 84.63 80.54\u201388.01 Kappa CNN 60.56 52.54\u201368.58 Inception-V3 (s) 70.33 62.97\u201377.69 Inception-V3 (m) 69.33 the accuracy of the CNN was above 80%, indicating good discrimination. higher AUC values are considered to demonstrate better discriminatory abilities as follows. acral melanoma is a pattern of acral melanoma, which is a ridge-and-furrow pattern. automated diagnosis methods are able to reflect experts\u2019 perception. deep learning does not require specific features as inputs. a large database is critical for the successful completion of deep learning. CNN would be helpful for the early detection of AM, which is usually associated with delayed diagnosis and poor prognosis."}