{"summary": "SL techniques with kernels are capable of addressing nonlinear problems without making parametric assumptions. but these techniques do not produce findings relevant for epidemiologic interpretations. a simulated case-control study was used to contrast the information embedding characteristics and separation boundaries produced by a specific SL technique. LR modeling is widely used for epidemiologic applications. but its separation boundary represents a latent characteristic that is often not considered directly. the LR model has an important epidemiologic interpretation. separation, or decision boundary, is a hyper-surface that reduces to a hyper-plane when x and the disease status bear a linear relation. in practice, decision models rarely produce perfect class-separation when making predictions. in practice, decision models rarely produce perfect class-separation when making predictions. non-parametric probabilistic methods can be used for converting SL outputs to more readily interpretable ORs. a SL technique comprised of the kernel mapping in combination with a perceptron NN [17] was compared with the LR modeling. simulated case-control dataset was generated with m = 200 observations in each of the case and control groups. both random variables (rvs) and their respective realizations are denoted by lower case letters, and vectors are similarly labeled with bold letters. each observation has two risk factors denoted by x1 and x2 expressed as a vector x = (x1, x2). to generate a given case-control dataset, 20,000 observations of (x1, x2) were generated randomly and processed with g(x1). the first m observations from each group were used to form a given case-control dataset resulting in 2m observations with equal numbers of cases and controls. the x1 observations were uniformly distributed rvs with unit variance. kernel mapping and perceptron combination will converge when the problem is well approximated a linear-separable. kernel operates on the risk factor vectors and eliminates the need to determine the general mapping function denoted by (x) the kernel operates on the risk factor vectors and eliminates the need to determine the general mapping function denoted by (x) each vector component difference has its own sigma-weight (1 and 2) that was determined with training methods discussed below. Equation (3) was used with both component terms (s1 = s2 =1) and with the individual component differences in isolation with (s1 = 1, s2 = 1) when the focus was on x2. resulting kernel elements form 2m 2m matrix, K, with elements k(xi, xj) = kij. a given row in the K matrix can be considered as new feature set (or row vector) for the respective observation (patient), which is the dimensionality expansion characteristic of the SL approach. perceptron processing used bootstrap methods to estimate in Eq. (8). the bias term b is not affected by the inputs [the kernel elements in Eq. (8)] but is an externally applied value (b = 1), left unchanged during the determination of the weight vector that fixes the position of the separation boundary (but does not affect the boundary orientation) the sigma-weights created two additional SL variants used in the comparison. the LR model was used with x1 and x2 without interaction. the SL approach and LR model required training to estimate the various parameters. perceptron weights were determined by drawing row vectors from the K (training) matrix at random with replacement. the sigma-weights were determined by the position of the maximum Az value. the perceptron weights that gave the highest Az before non-convergence were used in the validation processing along with 1, 2. a threshold was applied to each model's output and estimate its performance. in two class prediction problems, an operating point must be selected from the model output. this operating point represents a tradeoff between making two errors [13,14] a linear boundary gives x 2 = + 0 0 + 1 x 1 2 + 3 x 1, (12) the same approach was applied to the SL output. the SL technique output was modified to conform to the LR model interpretation and generate ORs. the components (p1 and pr) in Eq. (13) were constructed as approximations for continuous functions using non-parametric techniques. to estimate p1 and pr, first the histograms of normalized output scores for the m cases and m controls were analyzed separately. the disease status is dependent upon a given observation's x composition by this relation: g(x1) > x2. g(x1) assumes the position of the unknown function f(x) discussed above. 20,000 observations of (x1, x2) were generated randomly and processed with g(x1), which created the case-control designation. the x2 observations were generated by adding x1 to a normally distributed rv, designated as z1, with unit variance and mean = 5 giving x2= (x1+z1)/10. kernel mapping and perceptron combination will converge when the problem is well approximated a linear-separable. the kernel operates on the risk factor vectors and eliminates the need to specify the mapping function that provides for a linear separation boundary. the right side of Eq. (2) allows for the use of the left side without knowing the form of the right side. the kernel operation represents both a mapping of the input vectors [23] and also forms the basis for estimating probability density functions [26,27]. xj training sample is processed with every other xi sample using Eqs (3-4) to determine both the sigma-weights and the perceptron weight vector (i = 0 through 2m) the ith row of K results from the kernel operation of the sample with each of the other 2m samples indexed by j = 1 through 2m. i = j = 1 2 m j k ( x i, x j ) + b, (8) where j are the components of the new weight vector. the prospective observation's vector, x, is processed with the case-control training dataset consisting of 2m known risk factor vectors. the perceptron was also trained using x1 and x2 separately with the same procedure without regenerating the sigma-weights. to standardize the associations for the three SL models, the yest scores derived from Eq. (9) for a given model output were treated as single unit (both cases and control scores) the LR models were fitted with SAS (SAS Institute, NC) software. the SL approach required more involved training with bootstrap re-sampling. the training dataset was used to evaluate the fitted models. each boottrap dataset was processed by each of the models. the distribution mean (Az150) and standard deviation (150) were calculated for each model. for an arbitrary threshold value, pt, the separation boundary for the standard LR model was found by solving Eq. (10) for x2, giving x 2 = ( + 0 0 ) 2 1 2 x 1 (11) with 0 = ln ( 1 p t p t p t p t ). the same approach was applied to the SL output. kernel mapping and perceptron combination will converge when the problem is well approximated a linear-separable. the kernel operates on the risk factor vectors and eliminates the need to determine the general mapping function denoted by (x) the kernel operates on the risk factor vectors. xj given by D ( x, x j ) = s 1 ( x 1 x 1 j ) 2 1 2 + s 2 ( x 2 x 2 j ) 2 2. each vector component difference has its own sigma-weight (1 and 2). each component difference has its own sigma-weight (1 and 2) that was determined with training methods discussed below. resulting kernel elements form 2m 2m matrix, K, with elements k(xi, xj) = kij. a given row in the K matrix can be considered as new feature set (or row vector) for the respective observation (patient) perceptron processing used bootstrap methods [28] with the perceptron algorithm to estimate in Eq. (8). prospective observation's vector, x, is processed with the case-control training dataset consisting of 2m known risk factor vectors. perceptron was also trained using x1 and x2 separately with the same procedure without regenerating the sigma-weights. the SL approach required more involved training with bootstrap re-sampling. perceptron weights were determined by drawing row vectors from the K (training) matrix at random with replacement. the sigma-weights were determined by the position of the maximum Az value. the perceptron weights that gave the highest Az before non-convergence were used in the validation processing along with 1, 2. a threshold was applied to each model's output and estimate its performance. in two class prediction problems, an operating point must be selected from the model output, often derived from the ROC curve. this operating point represents a tradeoff between making two errors. a linear boundary gives x 2 = + 0 0 + 1 x 1 2 + 3 x 1, (12) the same approach was applied to the SL output. the method used to set the thresholds eliminated user input because there are an unlimited number of thresholds to choose from, each representing a different tradeoff. Equation (13) was generated for each of the SL variants for one of the evaluation datasets. the components (p1 and pr) in Eq. (13) were constructed as approximations for continuous functions. to estimate p1 and pr, first the histograms of normalized output scores for the m cases and m controls were analyzed separately. these large values are due to the unit increase because both x1 and x2 span less than one unit. x1 and x2 terms both show a positive association with the outcome while x2 shows a relatively stronger positive magnitude of association in comparison with x1. the two trained LR models and the three trained SL variants were used to process the 10 validation case-control datasets. the LR model gained marginal predictive capacity by adding the interaction term as indicated by the increased Az value. the univariate SL variants show that x1 in isolation contains considerable information content. the first 200 points correspond to controls and the next 200 points correspond to the cases. the respective normalized output scores are plotted on the vertical axis with the control scores denoted by multiplication signs and the case scores by diamonds. this figure shows the two risk factor scatter plot for the cases (diamonds) and controls (multiplication signs) for the LR model with x1 x2 interaction. each point represents a given sample's (x1, x2) risk vector plotted in component. the ORs were then derived by letting p1 = Pr(class = 1|z+z) with z = 0.10 (output-score increment units) the log(OR) plots for x1 and x2 individually, are shown in Figure 7 and Figure 8 respectively. the focus of the analysis is the OR nonlinearity. figure demonstrates the log (odds ratio) plot derived from the statistical learning method output, z, using x2 [s1 = 1 and s2 = 1 in Eq. (3)]. ln(odds ratio) plot derived from the statistical learning method output, z. the sigma-weight pair in combination with the perceptron weights that gave the highest Az were used in the comparison evaluation: (1, 2 ) = (3.88, 2.47). the trained model Az findings are given in Table 1 for the three SL models. the LR model gained marginal predictive capacity by adding the interaction term. the univariate SL variants show that x1 in isolation contains considerable information content in comparison with x2. the logistic regression model using x1 and x2 with x1 x2 interaction (LRint), the statistical learning (SL) model using a kernel mapping with x1 and x2 simultaneously (k), and partial SL-kernel models using x1 (kx1) and x2 (k2) individually. sensitivity = 0.80 and false positive fraction = 0.40 Figure 4. the SL output normalized z-scores for each sample are plotted on the vertical-axis. the SL output normalized z-scores for each sample are plotted on the vertical-axis. the separation boundary that gave 0.95 sensitivity is z = 0.49 (solid line) with a false positive fraction = 0.33. the log(ORs) derived from the SL outputs are constant, the Eq. (13) relations would approximate constant valued functions similar to the LR model coefficients. this figure demonstrates the numerical estimate of Pr(class = 1|z), where z is the statistical learning method output score using both risk factors. the SL output scores were transformed into ORs using a kernel density estimation technique. this transformation provided the essential link between the SL output and the epidemiologic interpretation for both the multivariate OR relation. the LR coefficient does not describe the association properly due to the LR model linear separation boundary. SL methods require more involved training than parametric modeling. for higher-dimensional problems more sophisticated optimization techniques are required. this scenario is seldom observed in real epidemiologic practice."}